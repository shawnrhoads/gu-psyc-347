{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you are using Anconda, please re-download this file to current directory:\n",
    "# https://raw.githubusercontent.com/shawnrhoads/gu-psyc-347/master/course-env.yml\n",
    "# Then, run this cell\n",
    "# !conda env update --file course-env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/shawnrhoads/gu-psyc-347/blob/master/docs/module-02-00_Linear-Modeling.ipynb\" target=\"_blank\">![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)</a>\n",
    "\n",
    "# Linear Modeling\n",
    "\n",
    "This tutorial was inspired by and adapted from the [Neuromatch Academy tutorials](https://github.com/NeuromatchAcademy/course-content) [[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of this tutorial\n",
    "1. [Specify a simple linear model](#what-is-a-linear-model)\n",
    "2. [Simulate a linear model](#model-simulations)\n",
    "3. [Gain an understanding about parameters (e.g., intercept, slope) in a linear model](#model-fitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a linear model?\n",
    "\n",
    "In its most basic form, a linear model can be written like this (e.g., a simple linear regression):\n",
    "\n",
    "$$\n",
    "y_n = intercept + b*x_n + \\epsilon_n\n",
    "$$\n",
    "\n",
    "where $y$ is the **dependent (outcome) variable** and $x$ is an **independent (explanatory/manipulated) variable**---these variables represent data (each participant $n$ has an observation at $x$, $y$). \n",
    "\n",
    "$y$ is a function of $x$ and is determined by 2 components:\n",
    "- a non-random component: $intercept + \\beta*x_n$\n",
    "- random component: $\\epsilon_n$\n",
    "\n",
    "The $intercept$ is the value of $y$ when $x=0$. $\\beta$ is a weighted parameter that determines the slope of the fitted linear model. We will determine the values of these parameters by fitting a linear model.\n",
    "\n",
    "The $error$ ($\\epsilon_n$) describes the random component of the linear relationship between $x$ and $y$---this is the difference between the true values ($y$) and the predicted values ($\\hat{y}$). We can solve this equation for the error:\n",
    "\n",
    "$$\n",
    "y_n = (intercept + \\beta*x_n) + \\epsilon_n\n",
    "$$\n",
    "$$\n",
    "y_n = \\hat{y_n} + \\epsilon_n\n",
    "$$\n",
    "$$\n",
    "\\epsilon_n = y_n - \\hat{y_n}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's begin by importing packages\n",
    "import pandas as pd, numpy as np, requests\n",
    "from scipy.stats import truncnorm\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Simulations\n",
    "Simulations are great ways to test models. By creating a simple synthetic dataset, we will know the true underlying model which allows us to see how our estimation efforts compare in uncovering the real model.\n",
    "\n",
    "Below, we will simulate a linear relationship between two variables `x` (antisocial behavior) and `y` (distance kept from others during the COVID-19 pandemic), and then we will add some \"noise\" to those data. This will help us gain a better understanding about expected results from [O'Connell, K., Berluti, K., Rhoads, S. A., & Marsh, A. A. (2021). Reduced social distancing during the COVID-19 pandemic is associated with antisocial behaviors in an online United States sample. PLoS ONE.](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0244974)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a fixed seed to our random number generator \n",
    "# ensures we will always\n",
    "# get the same psuedorandom number sequence\n",
    "np.random.seed(2021)\n",
    "\n",
    "# Let's set some parameters\n",
    "beta = -1.75\n",
    "n_subjects = 131\n",
    "\n",
    "# Draw x\n",
    "intercept = 347\n",
    "x0 = np.ones((n_subjects, 1))\n",
    "\n",
    "lower, upper = 32, 139 # range of STAB scores \n",
    "mu, sigma = 54.9, 30.1 # mean, and standard deviation\n",
    "x1 = truncnorm.rvs((lower-mu)/sigma,\n",
    "                   (upper-mu)/sigma,\n",
    "                   loc=mu,\n",
    "                   scale=sigma,\n",
    "                   size=(n_subjects,1))\n",
    "\n",
    "X = np.hstack((x0, x1))\n",
    "\n",
    "# sample from a standard normal distribution\n",
    "noise = np.random.normal(2, 10, (n_subjects,1)) \n",
    "\n",
    "# calculate y\n",
    "y = intercept + beta * x1 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(x1)\n",
    "ax.set(xlabel='Antisocial behavior', ylabel='Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(y)\n",
    "ax.set(xlabel='Distance from others', ylabel='Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x1, y)  # produces a scatter plot\n",
    "ax.set(xlabel='Antisocial behavior', ylabel='Distance from others')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our noisy dataset, we can try to estimate the underlying model that produced it. We use MSE to evaluate how successful a particular slope estimate $\\hat{\\beta}$ is for explaining the data, with the closer to 0 the MSE is, the better our estimate fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "Now, we will fit our simulated data to a simple linear regression, using least-squares optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinary least squares** is a very common optimization procedure that we are going to use for data fitting. \n",
    "\n",
    "Let's recall our simple linear model above. Suppose we have a set of measurements, $y_{n}$ (the \"dependent\" variable) obtained for different input values, $x_{n}$ (the \"independent\" or \"explanatory\" variable). Suppose we believe the measurements are proportional to the input values, but are corrupted by some (random) measurement errors, $\\epsilon_{n}$, that is:\n",
    "\n",
    "$$\\hat{y_{n}}= \\beta_0 + \\beta_1 x_{n}+\\epsilon_{n}$$\n",
    "\n",
    "for some unknown parameters ($\\beta_0$, $\\beta_1$). The least squares regression problem uses **mean squared error (MSE)** as its objective function, it aims to find the values of $\\beta_0$ and $\\beta_1$ by minimizing the average of squared errors:\n",
    "\n",
    "$$\n",
    "\\min _{\\beta} \\frac{1}{N}\\sum_{n=1}^{N}\\left(y_{n}-\\hat{y_{n}}\\right)^{2}\n",
    "$$\n",
    "\n",
    "We will now explore how MSE is used in fitting a linear regression model to data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing MSE\n",
    "\n",
    "In this exercise we will implement a method to compute the mean squared error (MSE) for a set of inputs $x$, measurements $y$, and slope estimates $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$. We will then compute and print the MSE for 3 different estimates of $\\hat{\\beta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(beta_hats, X, y):\n",
    "    \"\"\"Compute the mean squared error\n",
    "\n",
    "    Args:\n",
    "    beta_hats (list of floats): A list containing the estimate of the intercept parameter and the estimate of the slope parameter\n",
    "    X (ndarray): An array of shape (samples,) that contains the input values.\n",
    "    y (ndarray): An array of shape (samples,) that contains the corresponding measurement values to the inputs.\n",
    "\n",
    "    Returns:\n",
    "    y_hat (float): The estimated y_hat computed from X and the estimated parameter(s).\n",
    "    mse (float): The mean squared error of the model with the estimated parameter(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(beta_hats) == np.shape(X)[1]\n",
    "    \n",
    "    # Compute the estimated y\n",
    "    y_hat = 0\n",
    "    for index, b in enumerate(beta_hats):\n",
    "        y_hat += b*X[:,index]\n",
    "    \n",
    "    y_hat = y_hat.reshape(len(y_hat),1)\n",
    "    \n",
    "    # Compute mean squared error\n",
    "    mse = np.mean((y - y_hat)**2)\n",
    "\n",
    "    return y_hat, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 347\n",
    "possible_betas = [-0.75, -1.75, -2.75]\n",
    "\n",
    "for beta_hat in possible_betas:\n",
    "        \n",
    "    y_hat, MSE_val = mse([intercept, beta_hat], X, y)\n",
    "    print(f\"beta_hat of {beta_hat} has an MSE of {MSE_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(18, 4))\n",
    "\n",
    "for beta_hat, ax in zip(possible_betas, axes):\n",
    "\n",
    "    # True data\n",
    "    ax.scatter(x1, y, label='Observed')  # our data scatter plot\n",
    "\n",
    "    # Compute and plot predictions\n",
    "    y_hat, MSE_val = mse([intercept, beta_hat], X, y)\n",
    "\n",
    "    ax.plot(x1, y_hat, color='r', label='Fit')  # our estimated model\n",
    "\n",
    "    ax.set(\n",
    "        title= fr'$\\hat{{\\beta}}$= {beta_hat}, MSE = {MSE_val:.2f}',\n",
    "        xlabel='Anti-social behavior',\n",
    "        ylabel='Distance from others')\n",
    "    \n",
    "axes[0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters\n",
    "Using an interactive widget, we can easily see how changing intercept and slope estimates change a model fit. We display the **residuals** (differences between observed and predicted data) as line segments between the actual data and the corresponding predicted data on the model fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css\" integrity=\"sha512-5A8nwdMOWrSz20fDsjczgUidUBR8liPYU+WymTZP1lmY9G6Oc7HlZv156XqnsgNUzTyMefFTcsFH/tnJE/+xBg==\" crossorigin=\"anonymous\" />\n",
    "\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js\"></script>\n",
    "\n",
    "<script type=\"text/x-thebe-config\">\n",
    "     {\n",
    "       requestKernel: true,\n",
    "       binderOptions: {\n",
    "         repo: \"matplotlib/ipympl\",\n",
    "         ref: \"0.6.1\",\n",
    "         repoProvider: \"github\",\n",
    "       },\n",
    "     }\n",
    "</script>\n",
    "<script src=\"https://unpkg.com/thebe@latest/lib/index.js\"></script>\n",
    "   \n",
    "<button id=\"activateButton\" style=\"width: 450px; height: 40px; font-size: 1.5em;\">\n",
    "     Click here to interact with this visualization\n",
    "</button>\n",
    "\n",
    "<script>\n",
    "var bootstrapThebe = function() {\n",
    "   thebelab.bootstrap();\n",
    "}\n",
    "document.querySelector(\"#activateButton\").addEventListener('click', bootstrapThebe)\n",
    "</script>\n",
    "\n",
    "<pre data-executable=\"true\" data-language=\"python\">\n",
    "# interactive display\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "\n",
    "# set up data\n",
    "x0_viz = np.ones((131, 1))\n",
    "x1_norm = np.random.normal(loc=54.9,scale=30.1,size=(131,1))\n",
    "x1_viz = x1_norm - min(x1_norm)\n",
    "X_viz = np.hstack((x0_viz, x1_viz))\n",
    "noise_viz = np.random.normal(2, 10, (131,1)) \n",
    "y_viz = 347 - 1.75 * x1_viz + noise_viz\n",
    "\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "@widgets.interact(beta_hat=widgets.FloatSlider(-1.75, min=-3, max=0),\n",
    "                  intercept=widgets.FloatSlider(347, min=100, max=400))\n",
    "def plot_data_estimate(intercept, beta_hat):\n",
    "    # compute error\n",
    "    X, y = X_viz, y_viz\n",
    "    y_hat = (intercept*X[:,0] + beta_hat*X[:,1]).reshape(len(y),1)\n",
    "    MSE_val = np.mean((y - y_hat)**2)\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,1], y, label='Observed')  # our data scatter plot\n",
    "    ax.plot(X[:,1], y_hat, color='r', label='Fit')  # our estimated model\n",
    "\n",
    "    # plot residuals\n",
    "    ymin = np.minimum(y, y_hat)\n",
    "    ymax = np.maximum(y, y_hat)\n",
    "    ax.vlines(X[:,1], ymin, ymax, 'g', alpha=0.5, label='Residuals')\n",
    "\n",
    "    ax.set(\n",
    "        title=fr\"$intercept={intercept:0.2f}, \\hat{{\\beta}}$ = {beta_hat:0.2f}, MSE = {MSE_val:.2f}\",\n",
    "        xlabel='x',\n",
    "        ylabel='y')\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-squares minimization\n",
    "\n",
    "While the approach detailed above (computing MSE at various values of $\\hat\\beta$) quickly got us to a good estimate, it still relied on us guessing which beta values to select. If we didn't pick good guesses to begin with, we might miss the best possible parameter values.\n",
    "\n",
    "Thus, there must be a better way \"guess-timate\" them, right?\n",
    "\n",
    "Why don't we try an **exhaustive search** of across a specified range of parameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exhaustive search \n",
    "param_b0 = np.arange(325, 375, 1)\n",
    "param_b1 = np.arange(-3., 0., .25)\n",
    "\n",
    "first_run = 'first'\n",
    "mse_out = []\n",
    "for b0 in param_b0:\n",
    "    for b1 in param_b1:\n",
    "        if first_run is not None:\n",
    "            first_run = None\n",
    "            Params = [b0, b1]\n",
    "        else:\n",
    "            Params = np.vstack((Params, [b0, b1]))\n",
    "        mse_out = np.append(mse_out, mse([b0, b1], X, y)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From out exhaustive search:\n",
    "# let's see if we can recover the index with the smallest MSE\n",
    "min_index = np.argmin(mse_out)\n",
    "\n",
    "print(f'b0, b1 = {Params[min_index]}, MSE = {mse_out[min_index]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our fit is $b_0 = 349$ and $b_1 = -1.75$, which is quite close to our original!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also plot our search onto a heatmap!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_data = pd.DataFrame({'b0': Params[:,0],\n",
    "                                 'b1': Params[:,1], \n",
    "                                 'MSE': mse_out})\n",
    "data_pivoted = grid_search_data.pivot(\"b0\", \"b1\", \"MSE\")\n",
    "\n",
    "sns.heatmap(data_pivoted,\n",
    "            cmap='inferno',\n",
    "            cbar_kws={'label': 'mean squared error'})\n",
    "\n",
    "plt.title(\"Grid Search\\n(minimized MSE value is darkest on grid)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.optimize.minimize\n",
    "\n",
    "But, writing an exhaustive search by hand is prone to errors, too! It depends largely on the specified parameters for the search (i.e., we only sampled 300 to 450 in increments of .5 above for $b_0$).\n",
    "\n",
    "It can also be very time-consuming and computationally-expensive.\n",
    "\n",
    "Instead, we can utilize minimization algorithms that we optimized to solve minimization problems like this. Let's use the `scipy.optimize.minimize` function, which takes in the following inputs:\n",
    "- a cost function to minimize (`mse()` in our case)\n",
    "- starting points for the parameters to estimate (starting points for `b0` and `b1` in our case)\n",
    "- other arguments that need to be input into our objective function (`X` and `y` in our case)\n",
    "\n",
    "It will return an `OptimizeResult` object containing the estimated parameters (along with the value of minimized MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_to_minimize(beta_hats, X, y):\n",
    "    \"\"\"Compute the mean squared error\n",
    "\n",
    "    Args:\n",
    "    beta_hats (list of floats): A list containing the estimate of the intercept parameter and the estimate of the slope parameter\n",
    "    X (ndarray): An array of shape (samples,) that contains the input values.\n",
    "    y (ndarray): An array of shape (samples,) that contains the corresponding measurement values to the inputs.\n",
    "    \n",
    "    Returns:\n",
    "    mse (float): The mean squared error of the model with the estimated parameter(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(beta_hats) == np.shape(X)[1]\n",
    "    \n",
    "    # Compute the estimated y\n",
    "    y_hat = 0\n",
    "    for index, b in enumerate(beta_hats):\n",
    "        y_hat += b*X[:,index]\n",
    "    \n",
    "    y_hat = y_hat.reshape(len(y_hat),1)\n",
    "    \n",
    "    # Compute mean squared error\n",
    "    mse = np.mean((y - y_hat)**2)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observed_vs_predicted(beta_hats, X, y):\n",
    "    \"\"\" Plot observed vs predicted data\n",
    "\n",
    "    Args:\n",
    "    x (ndarray): observed x values\n",
    "    y (ndarray): observed y values\n",
    "    y_hat (ndarray): predicted y values\n",
    "    beta_hats (ndarray): An array of shape (betas,) that contains the estimate of the slope parameter(s)\n",
    "\n",
    "    \"\"\"\n",
    "    y_hat, MSE_val = mse(beta_hats, X, y)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,1], y, label='Observed')  # our data scatter plot\n",
    "    ax.plot(X[:,1], y_hat, color='r', label='Fit')  # our estimated model\n",
    "\n",
    "    # plot residuals\n",
    "    ymin = np.minimum(y, y_hat)\n",
    "    ymax = np.maximum(y, y_hat)\n",
    "    ax.vlines(X[:,1], ymin, ymax, 'g', alpha=0.5, label='Residuals')\n",
    "\n",
    "    ax.set(\n",
    "        title=fr\"$intercept={beta_hats[0]:0.2f}, \\hat{{\\beta}}$ = {beta_hats[1]:0.2f}, MSE = {MSE_val:.2f}\",\n",
    "        xlabel='x',\n",
    "        ylabel='y')\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# minimize MSE using scipy.optimize.minimize\n",
    "res = minimize(mse_to_minimize, # objective function\n",
    "               (350, -1.75), # estimated starting points\n",
    "               args=(X, y)) # arguments\n",
    "\n",
    "print(f'b0, b1 = {(res.x)}, MSE = {res.fun}')\n",
    "plot_observed_vs_predicted([res.x[0], res.x[1]], X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we can get a better fit: $b_0 = 352.01$ and $b_1 = -1.79$, which fairly accurate! Note that our **MSE** value is also smaller than last time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic solution for Least Squares Optimization (Bonus)\n",
    "\n",
    "While the approach detailed above (computing MSE at various values of $\\hat\\beta$) got us to a good estimate, we still had to rely on evaluating the MSE value across a grid of hand-specified values. \n",
    "\n",
    "If we don't pick a good range to begin with, or with enough granularity, we might miss the best possible parameters. Let's go one step further, and instead of finding the minimum MSE from a set of candidate estimates, let's solve for it analytically.\n",
    "\n",
    "We can do this by minimizing a \"cost function\". Mean squared error is a convex objective function, therefore we can compute its minimum using calculus (see appendix below for this derivation)! After computing the minimum, we find that:\n",
    "\n",
    "$$\n",
    "\\hat\\beta = \\frac{\\vec{x}^\\top \\vec{y}}{\\vec{x}^\\top \\vec{x}}\n",
    "$$\n",
    "\n",
    "This is known as solving the normal equation. For different ways of obtaining the solution, see the notes on [Least Squares Optimization](https://www.cns.nyu.edu/~eero/NOTES/leastSquares.pdf) by Eero Simoncelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ordinary_least_squares(X, y):\n",
    "    \"\"\"Ordinary least squares estimator for linear regression.\n",
    "\n",
    "    Args:\n",
    "    X (ndarray): design matrix of shape (n_samples, n_regressors)\n",
    "    y (ndarray): vector of measurements of shape (n_samples)\n",
    "\n",
    "    Returns:\n",
    "    ndarray: estimated parameter values of shape (n_regressors)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute theta_hat using OLS\n",
    "    beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    return beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute beta_hat\n",
    "beta_hat2 = ordinary_least_squares(X, y)\n",
    "print(f\"b0={beta_hat2[0]}, b1={beta_hat2[1]}\")\n",
    "\n",
    "# Compute MSE\n",
    "y_hat2 = X @ beta_hat2\n",
    "print(f\"MSE = {np.mean((y - y_hat2)**2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual data versus fitted data\n",
    "\n",
    "In addition to reviewing the residuals, we can also look at the actual simulated data (`y` or $y$) versus fitted data (`y_hat` or $\\hat{y}$). In other words, we can visualize the how the fitted values ($\\hat{y}$) compare to the actual data ($y$) to assess how well our model recovered our original parameters. We can also correlate $y$ with $\\hat{y}$ (a good model fit will show a strong correlation). This is helpful because we are often dealing with more than one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlate y and y_hat\n",
    "corrcoef = np.corrcoef(y.flatten(),y_hat2.flatten())[0,1]\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, y_hat2)  # produces a scatter plot\n",
    "ax.set(xlabel='y (simulated data + noise)', ylabel=fr'$\\haty$ (fitted data)')\n",
    "plt.title((f'r={corrcoef:.2f}'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Looks pretty nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to a statistics package\n",
    "Okay, cool!\n",
    "\n",
    "Let's see how our manual linear model fitting compares to a typical statistical packages. Let's import `scipy.stats` and run a simple linear regression with our observed data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stat_res = stats.linregress(X[:,1],y[:,0])\n",
    "\n",
    "y_hat_new = (stat_res.intercept + stat_res.slope*X[:,1]).reshape((len(y),1))\n",
    "\n",
    "print(f'b1 = {stat_res.slope:.2f} , b0 = {stat_res.intercept:.2f}, MSE = {np.mean((y - y_hat_new)**2):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate data\n",
    "Now that we have considered the univariate case, we turn to the general linear model case, where we can have more than one regressor, or feature, in our input.\n",
    "\n",
    "Recall that our original univariate linear model was given as\n",
    "\n",
    "$$\n",
    "y = \\beta x + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\beta$ is the slope and $\\epsilon$ some noise. We can easily extend this to the multivariate scenario by adding another parameter for each additional feature\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... +\\beta_d x_d + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\beta_0$ is the intercept and $d$ is the number of features (it is also the dimensionality of our input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will focus on the two-dimensional case ($d=2$), which allows us to easily visualize our results. \n",
    "\n",
    "In this case our model can be writen as:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add age\n",
    "np.random.seed(2021)\n",
    "\n",
    "b1 = -1.84\n",
    "b2 = -.68\n",
    "\n",
    "lower2, upper2 = 21, 65 # range of ages\n",
    "mu2, sigma2 = 36.34, 10.08 # mean and standard deviation\n",
    "x2 = np.random.normal(mu2, sigma2,\n",
    "                      size=(n_subjects,1))\n",
    "\n",
    "X_a = np.hstack((x0, x1, x2))\n",
    "\n",
    "# sample from a standard normal distribution\n",
    "noise = np.random.normal(2, 10, (n_subjects,1)) \n",
    "\n",
    "y_a = (b0 + b1*x1 + b2*x2 + noise).reshape((n_subjects,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute beta_hat\n",
    "beta_hat3 = ordinary_least_squares(X_a, y_a)\n",
    "print(f\"b0={beta_hat3[0]}, b1={beta_hat3[1]}, b2={beta_hat3[2]}\")\n",
    "\n",
    "# Compute MSE\n",
    "y_hat3 = (X_a @ beta_hat3)\n",
    "print(f\"MSE = {np.mean((y_a - y_hat3)**2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Ordinary least squares regression is an optimization procedure that can be used for fitting data to models (i.e., predicting a value for $y$ given $x$ by minimizing the MSE)\n",
    "\n",
    "**Note**: In this case, there is also an *analytical* solution to the minimization problem (see below). But as models become more complex, we will need to use a minimization algorithm to help us out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Least-squares derivation\n",
    "Here's the derivation of the least squares solution.\n",
    "\n",
    "First, set the derivative of the error with respect to $\\beta$ equal to zero (use chain rule),\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\beta}\\frac{1}{N}\\sum_{i=1}^N(y_i - \\beta x_i)^2 = 0 \\\\\n",
    "\\frac{1}{N}\\sum_{i=1}^N-2x_i(y_i - \\beta x_i) = 0\n",
    "$$\n",
    "\n",
    "Now solving for $\\beta$, we obtain an optimal value of:\n",
    "\n",
    "$$\n",
    "\\hat\\beta = \\frac{\\sum_{i=1}^N x_i y_i}{\\sum_{i=1}^N x_i^2}\n",
    "$$\n",
    "\n",
    "This can be written in vector notation as:\n",
    "\n",
    "$$\n",
    "\\hat\\beta = \\frac{\\vec{x}^\\top \\vec{y}}{\\vec{x}^\\top \\vec{x}}\n",
    "$$\n",
    "\n",
    "This is equivalent to the following code (used above): \n",
    "```\n",
    "beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "```\n",
    "\n",
    "This is known as solving the *normal equations*. For different ways of obtaining the solution, see the notes on [Least Squares Optimization](https://www.cns.nyu.edu/~eero/NOTES/leastSquares.pdf) by Eero Simoncelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://colab.research.google.com/github/shawnrhoads/gu-psyc-347/blob/master/docs/module-02-01_Nonlinear-Modeling.ipynb\">![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)</a>\n",
    "\n",
    "# Nonlinear Modeling\n",
    "\n",
    "This tutorial was inspired by and adapted from the [Neuromatch Academy tutorials](https://github.com/NeuromatchAcademy/course-content) [[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)], using a nonlinear hyperbolic model to assess social discounting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of this tutorial\n",
    "1. [Specifying a nonlinear model](#what-is-a-nonlinear-model)\n",
    "2. [Fitting data to a nonlinear model](#a-case-for-nonlinear-modeling-social-discounting)\n",
    "3. [Comparing models](#model-comparison)\n",
    "4. [Working with actual data](#fitting-actual-data-to-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "import numpy as np, pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a nonlinear model?\n",
    "Recall the general linear model, in which the multivariate relationship between a dependent variable ($y$) can be expressed as a linear combination of independent variables ($x_d$) that are multiplied by a weighted parameter or slope ($\\beta_d$), plus some noise ($\\epsilon$):\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... +\\beta_d x_d + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\beta_0$ is the intercept and $d$ is the number of features (it is also the dimensionality of our input).\n",
    "\n",
    "Nonlinear modeling simply implies that the relationship between $y$ and $x_d$ is more than a linear combination. Some common examples of nonlinear models include\n",
    "\n",
    "**Sigmoid function**: \n",
    "\n",
    "$y=\\frac{1}{1 + exp(\\beta_0 + \\beta_1x_1)}$\n",
    "\n",
    "**Exponential function**: \n",
    "\n",
    "$y = \\beta_0*exp(-\\beta_1x_1)$\n",
    "\n",
    "**Hyperbolic function**: \n",
    "\n",
    "$y=\\frac{\\beta_0}{1 + \\beta_1x_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot some of these examples:\n",
    "np.random.seed(2021)\n",
    "\n",
    "b0 = 1\n",
    "b1 = .04\n",
    "b2 = 1.5\n",
    "b3 = .0125\n",
    "b4 = 2.67\n",
    "\n",
    "x1 = np.random.normal(10, 20,\n",
    "                      size=(100,1))\n",
    "\n",
    "noise = np.random.randn(100).reshape((100,1))\n",
    "\n",
    "y = {'Linear': (b0 - b1*x1).reshape((100,1)),\n",
    "     'Sigmoid': ( ( 1 / (1 + np.exp(b2 + b4*x1)) )).reshape((100,1)),\n",
    "     'Exponential': (80*np.exp(-b1*x1)).reshape((100,1)),\n",
    "     'Hyperbolic': ((80/(1 + b3*x1))).reshape((100,1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, figsize=(18, 3))\n",
    "\n",
    "for (key, values), ax in zip(y.items(), axes):\n",
    "    \n",
    "    # True data\n",
    "    ax.scatter(x1, values)  # our data scatter plot\n",
    "\n",
    "    ax.set(title= fr'{key}')\n",
    "    \n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y', rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A case for nonlinear modeling: Social Discounting\n",
    "\n",
    "Now that we have a better understanding about some nonlinear models, let's see how we can apply them to understand people's prosocial preferences towards close others versus distant strangers. We will fit different models to understand the phenomenon known as **social discounting**, which describes how the amount of resources that individuals are willing to sacrifice for others ($v$) decreases as a hyperbolic function of social distance ($N$) [(Jones & Rachlin, 2006)](https://journals.sagepub.com/doi/10.1111/j.1467-9280.2006.01699.x).\n",
    "\n",
    "First, let's simulate some data to get a better intuition about the parameters in the models (the intercept $v_0$ and slope $k$):\n",
    "\n",
    "$$v=\\frac{v_0}{1 + k*N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2021)\n",
    "\n",
    "v0 = np.random.normal(85, 2,\n",
    "                      size=(100,))\n",
    "\n",
    "k = np.random.normal(.03, .005,\n",
    "                      size=(100,))\n",
    "\n",
    "N = np.array([1,2,5,10,20,50,100])\n",
    "\n",
    "v = []\n",
    "for i in range(100):\n",
    "    v.append((v0[i] / (1 + k[i]*N)) + noise[i]) \n",
    "    plt.plot(N, v[i], alpha=.5)\n",
    "\n",
    "plt.title('Social Discounting')\n",
    "plt.ylabel('Amount Willing to Forgo (v)')\n",
    "plt.xlabel('Social Distance (N)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "\n",
    "We have two \"free\" parameters in our hyperbolic model: the intercept ($v0$) and slope ($k$). The intercept ($v0$) represents the value of \"amount willing to forgo\" ($v$) when social distance ($N$) equals $0$. In other words, this is how much an individual values outcomes for him/herself (the undiscounted value of the outcomes)---a larger $v0$ would indicate that individuals value outcomes for themselves more (note that this parameter is somewhat difficult to interpret, so it is a common practice to add $+ 1$ to social distance so that the intercept represents the amount willing to forgo for $N=1$). The slope ($k$) measures the degree of discounting---a smaller $k$ describes more prosocial (or less selfish) choices for distant others while a larger $k$ describes more selfish (or less prosocial) choices for distant others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, let's create three functions in Python that compute $v$ (amount willing to forgo) as a function of $N$ (social distance). \n",
    "\n",
    "In both the exponential and hyperbolic case, we want to estimate the intercepts ($v_0$) and discounting rates ($k$) for each participant. We can compute these by minimizing the mean squared error (just like last time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_linear(params, N, v):\n",
    "    \n",
    "    v0, k = params\n",
    "    v_fit = v0 + k*N\n",
    "    \n",
    "    mse = np.mean((v - v_fit)**2)\n",
    "    \n",
    "    return mse   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_exponential(params, N, v):\n",
    "    \n",
    "    v0, k = params\n",
    "    v_fit = v0*np.exp(-k*N)\n",
    "    \n",
    "    mse = np.mean((v - v_fit)**2)\n",
    "    \n",
    "    return mse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_hyperbolic(params, N, v):\n",
    "    \n",
    "    v0, k = params\n",
    "    v_fit = (v0 / (1 + k*N))\n",
    "    \n",
    "    mse = np.mean((v - v_fit)**2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dictionary to store results\n",
    "results = {\"lin\": [],\n",
    "           \"exp\": [],\n",
    "           \"hyp\": []}\n",
    "\n",
    "# minimize MSE for linear function using scipy.optimize.minimize\n",
    "results[\"lin\"] = minimize(mse_linear, # objective function\n",
    "                          (85, -.3), # estimated starting points\n",
    "                          args=(N, v), # arguments\n",
    "                          bounds=((50,100),(-1,1)),\n",
    "                          tol=1e-3)\n",
    "\n",
    "# minimize MSE for hyperbolic function using scipy.optimize.minimize\n",
    "results[\"exp\"] = minimize(mse_exponential, # objective function\n",
    "                          (92, .6), # estimated starting points\n",
    "                          args=(N, v), # arguments\n",
    "                          bounds=((50,100),(0,1)),\n",
    "                          tol=1e-3)\n",
    "\n",
    "# minimize MSE for hyperbolic function using scipy.optimize.minimize\n",
    "results[\"hyp\"] = minimize(mse_hyperbolic, # objective function\n",
    "                          (70, .05), # estimated starting points\n",
    "                          args=(N, v), # arguments\n",
    "                          bounds=((50,100),(0,1)),\n",
    "                          tol=1e-3)\n",
    "\n",
    "print(f'Linear: v0 = {results[\"lin\"].x[0]:.2f}, k = {results[\"lin\"].x[1]:.3f}, MSE = {results[\"lin\"].fun:.2f}')\n",
    "print(f'Exponential: v0 = {results[\"exp\"].x[0]:.2f}, k = {results[\"exp\"].x[1]:.3f}, MSE = {results[\"exp\"].fun:.2f}')\n",
    "print(f'Hyperbolic: v0 = {results[\"hyp\"].x[0]:.2f}, k = {results[\"hyp\"].x[1]:.3f}, MSE = {results[\"hyp\"].fun:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(18, 4))\n",
    "for (model, fits), ax in zip(results.items(), axes):\n",
    "\n",
    "    # True data\n",
    "    v_mean = np.mean(v, axis=0)\n",
    "    ax.scatter(N, v_mean, \n",
    "               alpha = .75,\n",
    "               label = 'Observed (mean)')  # our data scatter plot\n",
    "    \n",
    "    v0, k = fits.x\n",
    "    mse_val = fits.fun\n",
    "\n",
    "    # Compute and plot predictions\n",
    "    if model == \"lin\":\n",
    "        v_hat = v0 + k * N\n",
    "    elif model == \"exp\":\n",
    "        v_hat = v0*np.exp(-k*N)\n",
    "    elif model == \"hyp\":\n",
    "        v_hat = v0 / (1 + k*N)\n",
    "        \n",
    "    ax.plot(N, v_hat, color='r', label='Fit (mean)')  # our estimated model\n",
    "    \n",
    "    # plot residuals\n",
    "    vmin = np.minimum(v_mean, v_hat)\n",
    "    vmax = np.maximum(v_mean, v_hat)\n",
    "\n",
    "    ax.vlines(N, vmin, vmax, 'g', alpha=0.5, label='Residuals')\n",
    "\n",
    "    ax.set(\n",
    "      title= fr'$k$= {k:.3f}, MSE = {mse_val:.2f}',\n",
    "      xlabel='N',\n",
    "      ylabel='v')\n",
    "    \n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[2].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of these model fits assume a **fixed** intercept ($v_o$) and **fixed** slope ($k$) across participants---in other words, these are the **average values** across the entire sample.\n",
    "\n",
    "In reality, we know that intercepts and slopes vary across participants, and can plot the differences below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all individual subjects\n",
    "for v_subj in v:\n",
    "    plt.plot(N, v_subj, alpha=.5)\n",
    "\n",
    "# plot mean model fit\n",
    "v_hat = results[\"hyp\"].x[0] / (1 + results[\"hyp\"].x[1]*N)        \n",
    "plt.plot(N, v_hat, \n",
    "         color='black', \n",
    "         linewidth=3, \n",
    "         label='Fit (mean)')  # our estimated model\n",
    "\n",
    "plt.title('Social Discounting')\n",
    "plt.ylabel('Amount Willing to Forgo (v)')\n",
    "plt.xlabel('Social Distance (N)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See small differences between fitted and observed values\n",
    "print(f'{v_hat} -- Fits')\n",
    "print(f'{v_mean} -- Observed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it might be better to make inferences with our data by allowing the intercepts and slopes to vary across participants. Fitting separate models for each participant is one way (of many) to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# initialize a DataFrame, with columns corresponding to params ['v0', k] and rows corresponding to subjects\n",
    "res_df = pd.DataFrame(columns=['v0', 'k'])\n",
    "\n",
    "for subj_id, subj_v in enumerate(v):\n",
    "    \n",
    "    # minimize MSE for hyperbolic function using scipy.optimize.minimize\n",
    "    fit = minimize(mse_hyperbolic, # objective function\n",
    "                   (85, .05), # estimated starting points\n",
    "                   args=(N, subj_v), # arguments\n",
    "                   bounds=((50,100),(0,1)),\n",
    "                   tol=1e-3)\n",
    "        \n",
    "    res_df.loc[subj_id, 'v0'] = fit.x[0]\n",
    "    res_df.loc[subj_id, 'k'] = fit.x[1]\n",
    "    res_df.loc[subj_id, 'MSE'] = fit.fun\n",
    "    \n",
    "    print(f'subject {subj_id}: v0 = {fit.x[0]:.2f}, k = {fit.x[1]:.3f}, MSE = {fit.fun:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the distributions of $v_0$ and $k$ across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(15,5))\n",
    "\n",
    "axes[0].hist(res_df['v0'], bins=10)\n",
    "axes[0].set(ylabel=\"Number of Subjects\", xlabel=\"v0\")\n",
    "\n",
    "axes[1].hist(res_df['k'], bins=10)\n",
    "axes[1].set(ylabel=\"Number of Subjects\", xlabel=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "Above, we can see that the hyperbolic model fits the data best, but typically the best fitting model isn't so obvious. Thus, we can use methods such as $R^2$ or the [Bayesian Information Criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) ($BIC$) to compare model fits. The model with the lowest $BIC$ value is the best fitting model in a finite set of models.\n",
    "\n",
    "The $BIC$ penalizes free parameters (see constant term: $k*ln(n)$):\n",
    "\n",
    "$$ BIC = -2*ln(MSE) + p*ln(n)$$\n",
    "\n",
    "Here, $n$ is the total number of observations in your sample (e.g., sample size), $p$ the number of parameters estimated by the model (we are estimating two parameters in our model: $v0$ and $k$), and $MSE$ is the mean sqauared error of the model. Remember that a lower $BIC$ value is better; Adding the term $p*ln(n)$ penalizes the model fit by the number of free parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bic(sample_size, mse, num_params):\n",
    "    bic = -2 * np.log(mse) + num_params * np.log(sample_size)\n",
    "    return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Bayesian Information Criterion (bic)\n",
    "for key, output in results.items():\n",
    "    bic = calculate_bic(len(v_mean), output.fun, 2)\n",
    "    print(f'{key} (BIC): {bic:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now show that the hyperbolic model is the model with the lowest $BIC$ value (out of the models tested above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting actual data to models\n",
    "\n",
    "Now that we've seen two examples of simulating data and model parameter recovery. Let's try to fit actual data to these models.\n",
    "\n",
    "We will use a subset of the data from [Vekaria et al. (2017)](https://www.nature.com/articles/s41562-017-0100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's load in the data\n",
    "\n",
    "# here, we are just going to download data from the web (no need to edit these lines, but try to figure out what they are doing)\n",
    "url = 'https://raw.githubusercontent.com/shawnrhoads/gu-psyc-347/master/docs/static/data/Vekaria-et-al-2017_data.csv'\n",
    "df = pd.read_csv(url, index_col='subject')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our data are formatted with participants as rows and amounts willing to forgo at each social distance as columns. To use our functions above, we will need to make sure our $v$ data have shape `(n_subjects, 7)`. \n",
    "\n",
    "Let's convert the pd.DataFrame to a np.array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vekaria_data = df.values\n",
    "\n",
    "print(vekaria_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's fit all of the data together, with fixed intercepts and slopes, for both the hyperbolic model and the exponential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit1 = minimize(mse_hyperbolic, # objective function\n",
    "               (85, .05), # estimated starting points\n",
    "               args=(N, df.iloc[:,0:7].values), # arguments\n",
    "               bounds=((0,80),(0,1)),\n",
    "               tol=1e-3)\n",
    "\n",
    "# minimize MSE for exponential function using scipy.optimize.minimize\n",
    "fit2 = minimize(mse_exponential, # objective function\n",
    "               (85, .05), # estimated starting points\n",
    "               args=(N, df.iloc[:,0:7].values), # arguments\n",
    "               bounds=((0,80),(0,1)),\n",
    "               tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots()\n",
    "fig = plt.figure()    \n",
    "\n",
    "plt.scatter(N, np.mean(df.iloc[:,0:7].values, axis=0), label='Observed (mean)')\n",
    "plt.plot(N, fit1.x[0] / (1 + fit1.x[1]*N), label='Hyperbolic')\n",
    "plt.plot(N, fit2.x[0] * np.exp(-fit1.x[1]*N), label='Exponential')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this plot, we can clearly see how much better the hyperbolic model is at explaining the variance in the data. We can confirm this again using the $BIC$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Bayesian Information Criterion (bic)\n",
    "for label, output in zip(['hyperbolic', 'exponential'], [fit1, fit2]):\n",
    "    bic = calculate_bic(len(vekaria_data), output.fun, 2)\n",
    "    print(f'{label} (BIC): {bic:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# initialize a DataFrame, with columns corresponding to params ['v0', k] and rows corresponding to subjects\n",
    "res_vekaria = pd.DataFrame(columns=['v0', 'k'])\n",
    "\n",
    "for subj_id, subj_v in zip(df.index, vekaria_data):\n",
    "    \n",
    "    # minimize MSE for hyperbolic function using scipy.optimize.minimize\n",
    "    fit = minimize(mse_hyperbolic, # objective function\n",
    "                   (85, .05), # estimated starting points\n",
    "                   args=(N, subj_v), # arguments\n",
    "                   bounds=((0,80),(0,1)),\n",
    "                   tol=1e-3)\n",
    "        \n",
    "    res_vekaria.loc[subj_id, 'v0'] = fit.x[0]\n",
    "    res_vekaria.loc[subj_id, 'k'] = fit.x[1]\n",
    "    res_vekaria.loc[subj_id, 'MSE'] = fit.fun\n",
    "    \n",
    "    print(f'subject {subj_id}: v0 = {fit.x[0]:.2f}, k = {fit.x[1]:.3f}, MSE = {fit.fun:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some participants did not do very well with model fitting. For most, this is because their \"amounts willing to forgo\" do not vary across social distances.\n",
    "\n",
    "To account for this, let's check which subjects these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj_id, subj_v in zip(df.index, vekaria_data):\n",
    "    if all(x==subj_v[0] for x in subj_v):\n",
    "        print(f'no variation for subject #{subj_id}, {subj_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three participants sacrificed all of their resources for all social others. Let's assign `k=0` and `v0=85` to these participants since there is no variation in their preferences. This is eqivalent to a straight horizontal line (no discounting) at $v=85$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a DataFrame, with columns corresponding to params ['v0', k] and rows corresponding to subjects\n",
    "hyp_vekaria = pd.DataFrame(columns=['v0', 'k'])\n",
    "\n",
    "for subj_id, subj_v in zip(df.index, vekaria_data):\n",
    "    \n",
    "    if all(x==subj_v[0] for x in subj_v):\n",
    "        if np.sum(subj_v)>=595:\n",
    "            \n",
    "            hyp_vekaria.loc[subj_id, 'v0'] = 80 # \n",
    "            hyp_vekaria.loc[subj_id, 'k'] = 0\n",
    "            hyp_vekaria.loc[subj_id, 'MSE'] = np.nan\n",
    "            \n",
    "            print(f'assigning k=0 for subject #{subj_id}, {subj_v}')\n",
    "    else:\n",
    "    \n",
    "        # minimize MSE for hyperbolic function using scipy.optimize.minimize\n",
    "        fit = minimize(mse_hyperbolic, # objective function\n",
    "                       (85, .05), # estimated starting points\n",
    "                       args=(N, subj_v), # arguments\n",
    "                       bounds=((0,80),(0,1)),\n",
    "                       tol=1e-3)\n",
    "\n",
    "        hyp_vekaria.loc[subj_id, 'v0'] = fit.x[0]\n",
    "        hyp_vekaria.loc[subj_id, 'k'] = fit.x[1]\n",
    "        hyp_vekaria.loc[subj_id, 'MSE'] = fit.fun\n",
    "\n",
    "res_vekaria = pd.concat([df, hyp_vekaria], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(15,5), sharey=True)\n",
    "\n",
    "axes[0].hist(res_vekaria['v0'], bins=20)\n",
    "axes[0].set(ylabel=\"Number of Subjects\", xlabel=\"v0\")\n",
    "\n",
    "axes[1].hist(res_vekaria['k'], bins=20)\n",
    "axes[1].set(ylabel=\"Number of Subjects\", xlabel=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Now, we can use these data for subsequent analyses! Note the little variation in $v_0$. Also note that $k$ is not parametric (e.g., not normally distributed), so we would need to conduct subsequent analyses using non-parametric approaches."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "promotional-viewer",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://colab.research.google.com/github/shawnrhoads/gu-psyc-347/blob/master/docs/module-04-01_Prosocial-RL-Exercises.ipynb\">![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)</a>\n",
    "\n",
    "# Prosocial RL Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-trunk",
   "metadata": {},
   "source": [
    "We will now fit your behavioral data to the models developed during class (as well as a few others) and then compare the models to assess which model best explains the data. This exercise is a bit different from previous exercises. I will only provide you with minimal code/text to guide you through, but you should be able to complete it all on your own using the course resources, previous tutorials/exercises, previous papers, and previous lectures!\n",
    "\n",
    "**Once you complete this exercise, you will be well on your way to becoming a pro computational modeler!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-smoke",
   "metadata": {},
   "source": [
    "<hr width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-bosnia",
   "metadata": {},
   "source": [
    "Please download the data from [GitHub](https://github.com/shawnrhoads/gu-psyc-347/tree/master/docs/static/data), Canvas, or by using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "\n",
    "# create list of URLs\n",
    "urls = [f'https://raw.githubusercontent.com/shawnrhoads/gu-psyc-347/master/docs/static/data/{x:02}_psyc-347-prosocial-learning.csv' for x in range(1,12)]\n",
    "\n",
    "# loop through list and download data\n",
    "for url in urls:\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    if os.path.isdir('static/data'):\n",
    "        filename = 'static/data/' + os.path.basename(url)\n",
    "    else:\n",
    "        filename = os.path.basename(url)\n",
    "    open(filename, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-desktop",
   "metadata": {},
   "source": [
    "Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize # finding optimal params in models\n",
    "from scipy import stats             # statistical tools\n",
    "import os                           # operating system tools\n",
    "import numpy as np                  # matrix/array functions\n",
    "import pandas as pd                 # loading and manipulating data\n",
    "import matplotlib.pyplot as plt     # plotting\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2021)                # set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will load the data into memory (assuming that\n",
    "# the data are downloaded)\n",
    "def load_subjects():\n",
    "    '''\n",
    "    input: n/a\n",
    "    output: dictionary of DataFrames containing the data\n",
    "    '''\n",
    "        \n",
    "    if os.path.isdir('static/data'):\n",
    "        files = [f'static/data/{x:02}_psyc-347-prosocial-learning.csv' for x in range(1,12)]\n",
    "    else:\n",
    "        files = [f'{x:02}_psyc-347-prosocial-learning.csv' for x in range(1,12)]\n",
    "    \n",
    "    subject_data = {}\n",
    "    for index, file in enumerate(files):\n",
    "        df = pd.read_csv(file, index_col='subject')\n",
    "        subject_data[index] = df_filtered = df[df['outcomeDescr'] != 'practice'][['block',\n",
    "                                                                                  'trial_num',\n",
    "                                                                                  'true_accuracy',\n",
    "                                                                                  'outcome',\n",
    "                                                                                  'outcomeDescr',\n",
    "                                                                                  'cumulativePts_self',\n",
    "                                                                                  'cumulativePts_social']]\n",
    "        \n",
    "    return subject_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load in our data using the function above\n",
    "# feel free to adjust `how_many` (the default is all 11 subjects)\n",
    "subject_data = load_subjects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-mustang",
   "metadata": {},
   "source": [
    "## Relevant Data \n",
    "\n",
    "#### 4 conditions/blocks\n",
    "- self, win\n",
    "- self, avoid loss\n",
    "- social, win\n",
    "- social, avoid loss\n",
    "\n",
    "#### Each block contains 24 trials\n",
    "- 2 stimuli per trial (counterbalanced wrt side)\n",
    "- outcomes associated with each stimulus is probabilistic (75%, 25%)\n",
    "\n",
    "#### Data output key\n",
    "- `block`: (play for self to win); (play for self to avoid loss); (play for next student to win); (play for next student to win)\n",
    "- `trial_num`: order in which trials are displayed within a block (0-23)\n",
    "- `true_accuracy`: 1 if selected stimulus with highest probability of winning or avoid losing\n",
    "- `outcome`: outcome for trial (`+1`, `0`, `-1`); blank if missed\n",
    "- `outcomeDescr`: text description of outcome (practice, social win, social avoid win, social loss, social avoidloss, self win, self avoid win, self loss, self avoidloss)\n",
    "- `cumulativePts_self`: running total of points for self\n",
    "- `cumulativePts_social`: running total of points for social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view data from first subject\n",
    "display(subject_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-ensemble",
   "metadata": {},
   "source": [
    "<hr width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_positions(list_of_elems, element):\n",
    "    ''' Returns the indexes of all occurrences of give element in\n",
    "    the list- listOfElements '''\n",
    "    index_pos_list = []\n",
    "    index_pos = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # Search for item in list from indexPos to the end of list\n",
    "            index_pos = list_of_elems.index(element, index_pos)\n",
    "            # Add the index position in list\n",
    "            index_pos_list.append(index_pos)\n",
    "            index_pos += 1\n",
    "        except ValueError as e:\n",
    "            break\n",
    "            \n",
    "    return index_pos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-running",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completed\n",
    "def Simple_1a1t(params, choices, outcomes, block, plot=False):\n",
    "    '''\n",
    "    Inputs:\n",
    "        - params: list of 2 guesses (float) for each parameter (alpha, theta)\n",
    "        - choices: list of 96 choices (int) on each trial (0, 1)\n",
    "        - outcomes: list of 96 outcomes (int) on each trial (-1, 0, 1)\n",
    "        - block: list of 96 conditions (string) on each trial (self_win, self_avoidloss, social_win, social_avoidloss)\n",
    "    Outputs:\n",
    "        - negLL: negative loglikelihood computed\n",
    "                 from the choice probabilities (float)\n",
    "    '''\n",
    "    alpha, theta = params \n",
    "    \n",
    "    if np.isnan(alpha) or np.isnan(theta): # check inputs\n",
    "        return np.inf\n",
    "    else:\n",
    "        blocks = list(block)\n",
    "\n",
    "        # extracts list of four strings corresponding to conditions\n",
    "        unique_conditions = list(set(block))\n",
    "\n",
    "        # init choice probs\n",
    "        choiceProb = np.zeros((len(blocks)), dtype = float) \n",
    "        Q_out = {}\n",
    "\n",
    "        count = 0\n",
    "        for condition in unique_conditions:\n",
    "\n",
    "            T_temp = blocks.count(condition)    \n",
    "            Q = [0.5, 0.5] # Q at trial 0\n",
    "            Q_stored = np.zeros((2, T_temp), dtype = float) \n",
    "\n",
    "            cur_indices = get_index_positions(blocks, condition)\n",
    "            c, r = np.array(choices)[cur_indices], np.array(outcomes)[cur_indices]\n",
    "\n",
    "            # check if self vs social\n",
    "            if 'self' in condition:\n",
    "                # check if win vs avoid loss\n",
    "                if 'win' in condition:\n",
    "                    r = r\n",
    "                elif 'avoidloss' in condition:\n",
    "                    r = np.array([x+1 for x in r])                \n",
    "            elif 'social' in condition:\n",
    "                # check if win vs avoid loss\n",
    "                if 'win' in condition:\n",
    "                    r = r\n",
    "                elif 'avoidloss' in condition:\n",
    "                    r = np.array([x+1 for x in r])\n",
    "            \n",
    "            # loop through trials within condition\n",
    "            for t in range(T_temp):       \n",
    "\n",
    "                if np.isnan(c[t]):\n",
    "                    #don't update\n",
    "                    choiceProb[count] = np.nan\n",
    "                    Q_stored[:,t] = Q\n",
    "\n",
    "                else:\n",
    "                    # compute choice probabilities for k=2\n",
    "                    # use the softmax rule\n",
    "                    ev = np.exp(theta*np.array(Q))\n",
    "                    sum_ev = np.sum(ev)\n",
    "                    p = ev / sum_ev\n",
    "\n",
    "                    # compute choice probability for actual choice\n",
    "                    choiceProb[count] = p[int(c[t])]\n",
    "\n",
    "                    # update values\n",
    "                    delta = r[t] - Q[int(c[t])]\n",
    "                    Q[int(c[t])] = Q[int(c[t])] + alpha * delta\n",
    "\n",
    "                    # store Q_t+1\n",
    "                    Q_stored[:,t] = Q\n",
    "\n",
    "                count += 1\n",
    "\n",
    "            #plt.plot(range(T_temp),Q_stored[0,:])\n",
    "            #plt.plot(range(T_temp),Q_stored[1,:])\n",
    "            #plt.show()\n",
    "            Q_out[condition] = Q_stored\n",
    "            \n",
    "        negLL = -np.nansum(np.log(choiceProb))\n",
    "        \n",
    "        if plot: #plot mean across 4 blocks\n",
    "            Q0 = np.mean(np.stack((Q_out['self_win'][0], Q_out['social_win'][0], Q_out['self_avoidloss'][0], Q_out['social_avoidloss'][0]),axis=0),axis=0)\n",
    "            Q1 = np.mean(np.stack((Q_out['self_win'][1], Q_out['social_win'][1], Q_out['self_avoidloss'][1], Q_out['social_avoidloss'][1]),axis=0),axis=0)\n",
    "\n",
    "            plt.plot(range(T_temp),Q0)\n",
    "            plt.plot(range(T_temp),Q1)\n",
    "            plt.title('Mean Q across conditions')\n",
    "            plt.xlabel('trial')\n",
    "            plt.ylabel('Q')\n",
    "            plt.show()\n",
    "        \n",
    "        return negLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-officer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# example for testing function\n",
    "behavior = subject_data[9]\n",
    "params = [.12,2.11]\n",
    "choices, outcomes, block = behavior.true_accuracy, behavior.outcome, behavior.block\n",
    "subj_negll = Simple_1a1t(params, choices, outcomes, block, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completed\n",
    "def fitModel0(model=Simple_1a1t):\n",
    "    \n",
    "    #initialize dataframe to store results\n",
    "    df = pd.DataFrame(index=range(1,12), columns=['alpha','theta','NLL'])\n",
    "\n",
    "    # initialize list of algorithms \n",
    "    # (use one during class, but try a few others on your own)\n",
    "    algorithms = ['L-BFGS-B'] #['Powell','TNC','SLSQP','trust-constr']\n",
    "\n",
    "    # loop through subjects\n",
    "    for index, behavior in enumerate(subject_data.values()):\n",
    "        c, o, block = behavior.true_accuracy, behavior.outcome, behavior.block\n",
    "        bounds = ((0,1),(0,10))\n",
    "\n",
    "        # gradient descent to minimize neg LL\n",
    "        res_nll = np.inf # set initial neg LL to be inf\n",
    "\n",
    "        # guess several different starting points for alpha\n",
    "        for alpha_guess in np.linspace(0,1,6):\n",
    "            for theta_guess in np.linspace(1,10,6):\n",
    "\n",
    "                # guesses for alpha, theta will change on each loop\n",
    "                init_guess = (alpha_guess, theta_guess)\n",
    "\n",
    "                for algorithm in algorithms:\n",
    "\n",
    "                    # minimize neg LL\n",
    "                    result = minimize(model,\n",
    "                                      init_guess,\n",
    "                                      (c, o, block),\n",
    "                                      bounds=bounds,\n",
    "                                      method=algorithm)\n",
    "\n",
    "                    # if current negLL is smaller than the last negLL,\n",
    "                    # then store current data\n",
    "                    if result.fun < res_nll and result.success:\n",
    "                        res_nll = result.fun\n",
    "                        param_fits = result.x\n",
    "\n",
    "        # also, compute BIC\n",
    "        BIC = len(init_guess) * np.log(len(c)) + 2*res_nll\n",
    "\n",
    "        #store in dataframe\n",
    "        df.at[index, 'alpha'] = param_fits[0]\n",
    "        df.at[index, 'theta'] = param_fits[1]\n",
    "        df.at[index, 'NLL'] = res_nll\n",
    "\n",
    "        # print/plot Q values for subject   \n",
    "        print(fr'subject {index+1:02}: alpha={param_fits[0]:.2f}, theta={param_fits[1]:.2f}; negLL={res_nll:.2f}; BIC={BIC:.2f}')\n",
    "        nll = model(param_fits, c, o, block, plot=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-table",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to run\n",
    "# df0a = fitModel0(model=Simple_1a1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-dream",
   "metadata": {},
   "source": [
    "<hr width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completed\n",
    "def Social_1a1t(params, choices, outcomes, block, plot=False):\n",
    "    # 1 alpha_self (where alpha_other = 1 - alpha_self) + 1 theta\n",
    "    '''\n",
    "    Inputs:\n",
    "        - params: list of 2 guesses (float) for each parameter (alpha, theta)\n",
    "        - choices: list of 96 choices (int) on each trial (0, 1)\n",
    "        - outcomes: list of 96 outcomes (int) on each trial (-1, 0, 1)\n",
    "        - block: list of 96 conditions (string) on each trial (self_win, self_avoidloss, social_win, social_avoidloss)\n",
    "    Outputs:\n",
    "        - negLL: negative loglikelihood computed\n",
    "                 from the choice probabilities (float)\n",
    "    '''\n",
    "    alpha_self, theta = params \n",
    "    alpha_other = 1 - alpha_self\n",
    "    \n",
    "    if np.isnan(alpha_self) or np.isnan(theta): # check inputs\n",
    "        return np.inf\n",
    "    else:\n",
    "        blocks = list(block)\n",
    "\n",
    "        # extracts list of four strings corresponding to conditions\n",
    "        unique_conditions = list(set(block))\n",
    "\n",
    "        # init choice probs\n",
    "        choiceProb = np.zeros((len(blocks)), dtype = float) \n",
    "        Q_out = {}\n",
    "\n",
    "        count = 0\n",
    "        for condition in unique_conditions:\n",
    "\n",
    "            T_temp = blocks.count(condition)    \n",
    "            Q = [0.5, 0.5] # Q at trial 0\n",
    "            Q_stored = np.zeros((2, T_temp), dtype = float) \n",
    "\n",
    "            cur_indices = get_index_positions(blocks, condition)\n",
    "            c, r = np.array(choices)[cur_indices], np.array(outcomes)[cur_indices]\n",
    "\n",
    "            # check if self vs social\n",
    "            if 'self' in condition:\n",
    "                # check if win vs avoid loss\n",
    "                if 'win' in condition:\n",
    "                    r = r\n",
    "                elif 'avoidloss' in condition:\n",
    "                    r = np.array([x+1 for x in r]) \n",
    "                \n",
    "                \n",
    "            elif 'social' in condition:\n",
    "                # check if win vs avoid loss\n",
    "                if 'win' in condition:\n",
    "                    r = r\n",
    "                elif 'avoidloss' in condition:\n",
    "                    r = np.array([x+1 for x in r])\n",
    "            \n",
    "            # loop through trials within condition\n",
    "            for t in range(T_temp):       \n",
    "\n",
    "                if np.isnan(c[t]):\n",
    "                    #don't update\n",
    "                    choiceProb[count] = np.nan\n",
    "                    Q_stored[:,t] = Q\n",
    "\n",
    "                else:\n",
    "                    # compute choice probabilities for k=2\n",
    "                    # use the softmax rule\n",
    "                    ev = np.exp(theta*np.array(Q))\n",
    "                    sum_ev = np.sum(ev)\n",
    "                    p = ev / sum_ev\n",
    "\n",
    "                    # compute choice probability for actual choice\n",
    "                    choiceProb[count] = p[int(c[t])]\n",
    "\n",
    "                    # update values\n",
    "                    delta = r[t] - Q[int(c[t])]\n",
    "                    \n",
    "                    if 'self' in condition:\n",
    "                        Q[int(c[t])] = Q[int(c[t])] + alpha_self * delta\n",
    "                    elif 'social' in condition:\n",
    "                        Q[int(c[t])] = Q[int(c[t])] + alpha_other * delta\n",
    "\n",
    "                    # store Q_t+1\n",
    "                    Q_stored[:,t] = Q\n",
    "\n",
    "                count += 1\n",
    "\n",
    "            #plt.plot(range(T_temp),Q_stored[0,:])\n",
    "            #plt.plot(range(T_temp),Q_stored[1,:])\n",
    "            #plt.show()\n",
    "            Q_out[condition] = Q_stored\n",
    "            \n",
    "        negLL = -np.nansum(np.log(choiceProb))\n",
    "        \n",
    "        if plot: #plot mean across 4 blocks\n",
    "            Q0 = np.mean(np.stack((Q_out['self_win'][0], Q_out['social_win'][0], Q_out['self_avoidloss'][0], Q_out['social_avoidloss'][0]),axis=0),axis=0)\n",
    "            Q1 = np.mean(np.stack((Q_out['self_win'][1], Q_out['social_win'][1], Q_out['self_avoidloss'][1], Q_out['social_avoidloss'][1]),axis=0),axis=0)\n",
    "\n",
    "            plt.plot(range(T_temp),Q0)\n",
    "            plt.plot(range(T_temp),Q1)\n",
    "            plt.title('Mean Q across conditions')\n",
    "            plt.xlabel('trial')\n",
    "            plt.ylabel('Q')\n",
    "            plt.show()\n",
    "        \n",
    "        return negLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-garbage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to run\n",
    "# df0b = fitModel0(model=Social_1a1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-botswana",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-retirement",
   "metadata": {},
   "source": [
    "## Fitting your own models!\n",
    "\n",
    "Each group will write code for custom models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model1 (8 out of 30 points)\n",
    "def Social_2a1b1t(params, choices, outcomes, block, plot=False):\n",
    "    # 1 alpha_self + 1 alpha_other + 1 beta + 1 theta\n",
    "    \n",
    "    return negLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completed\n",
    "def fitModel1(model=Social_2a1b1t):\n",
    "    \n",
    "    #initialize dataframe to store results\n",
    "    df = pd.DataFrame(index=range(1,12), columns=['alpha_self','alpha_other','beta','theta','NLL'])\n",
    "\n",
    "    # initialize list of algorithms \n",
    "    # (use one during class, but try a few others on your own)\n",
    "    algorithms = ['L-BFGS-B'] #['Powell','TNC','SLSQP','trust-constr']\n",
    "\n",
    "    # loop through subjects\n",
    "    for index, behavior in enumerate(subject_data.values()):\n",
    "        c, o, block = behavior.true_accuracy, behavior.outcome, behavior.block\n",
    "        bounds = ((0,1),(0,1),(0,1),(0,10))\n",
    "\n",
    "        # gradient descent to minimize neg LL\n",
    "        res_nll = np.inf # set initial neg LL to be inf\n",
    "\n",
    "        # guess several different starting points for alpha\n",
    "        for alpha_self_guess in np.linspace(0,1,6):\n",
    "            for alpha_other_guess in np.linspace(0,1,6):\n",
    "                for beta_guess in np.linspace(0,1,6):\n",
    "                    for theta_guess in np.linspace(1,10,6):\n",
    "\n",
    "                        # guesses for alpha, theta will change on each loop\n",
    "                        init_guess = (alpha_self_guess, \n",
    "                                      alpha_other_guess, \n",
    "                                      beta, \n",
    "                                      theta_guess)\n",
    "\n",
    "                        for algorithm in algorithms:\n",
    "\n",
    "                            # minimize neg LL\n",
    "                            result = minimize(model,\n",
    "                                              init_guess,\n",
    "                                              (c, o, block),\n",
    "                                              bounds=bounds,\n",
    "                                              method=algorithm)\n",
    "\n",
    "                            # if current negLL is smaller than the last negLL,\n",
    "                            # then store current data\n",
    "                            if result.fun < res_nll and result.success:\n",
    "                                res_nll = result.fun\n",
    "                                param_fits = result.x\n",
    "\n",
    "        # also, compute BIC\n",
    "        BIC = len(init_guess) * np.log(len(c)) + 2*res_nll\n",
    "\n",
    "        #store in dataframe\n",
    "        df.at[index, 'alpha_self'] = param_fits[0]\n",
    "        df.at[index, 'alpha_other'] = param_fits[1]\n",
    "        df.at[index, 'beta'] = param_fits[2]\n",
    "        df.at[index, 'theta'] = param_fits[3]\n",
    "        df.at[index, 'NLL'] = res_nll\n",
    "\n",
    "        # print/plot Q values for subject   \n",
    "        print(fr'subject {index+1:02}: alpha={param_fits[0]:.2f}, theta={param_fits[1]:.2f}; negLL={res_nll:.2f}; BIC={BIC:.2f}')\n",
    "        nll = model(param_fits, c, o, block, plot=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run\n",
    "# df1 = fitModel1(model=Social_2a1b1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-sector",
   "metadata": {},
   "source": [
    "<hr width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model2 (8 out of 30 points)\n",
    "def Valence_2a1t(params, choices, outcomes, block):\n",
    "    # 1 alpha_positive + 1 alpha_negative + 1 theta\n",
    "    \n",
    "    return negLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel2(model=Valence_2a1t):\n",
    "    \n",
    "    #initialize dataframe to store results\n",
    "    df = pd.DataFrame(index=range(1,12), columns=['alpha_pos','alpha_neg','theta','NLL'])\n",
    "\n",
    "    # initialize list of algorithms \n",
    "    # (use one during class, but try a few others on your own)\n",
    "    algorithms = ['L-BFGS-B'] #['Powell','TNC','SLSQP','trust-constr']\n",
    "\n",
    "    # loop through subjects\n",
    "    for index, behavior in enumerate(subject_data.values()):\n",
    "        c, o, block = behavior.true_accuracy, behavior.outcome, behavior.block\n",
    "        bounds = ((0,1),(0,1),(0,10))\n",
    "\n",
    "        # gradient descent to minimize neg LL\n",
    "        res_nll = np.inf # set initial neg LL to be inf\n",
    "\n",
    "        # guess several different starting points for alpha\n",
    "        for alpha_pos_guess in np.linspace(0,1,6):\n",
    "            for alpha_neg_guess in np.linspace(0,1,6):\n",
    "                for theta_guess in np.linspace(1,10,6):\n",
    "\n",
    "                    # guesses for alpha, theta will change on each loop\n",
    "                    init_guess = (alpha_pos_guess, \n",
    "                                  alpha_neg_guess, \n",
    "                                  theta_guess)\n",
    "\n",
    "                    for algorithm in algorithms:\n",
    "\n",
    "                        # minimize neg LL\n",
    "                        result = minimize(model,\n",
    "                                          init_guess,\n",
    "                                          (c, o, block),\n",
    "                                          bounds=bounds,\n",
    "                                          method=algorithm)\n",
    "\n",
    "                        # if current negLL is smaller than the last negLL,\n",
    "                        # then store current data\n",
    "                        if result.fun < res_nll and result.success:\n",
    "                            res_nll = result.fun\n",
    "                            param_fits = result.x\n",
    "\n",
    "        # also, compute BIC\n",
    "        BIC = len(init_guess) * np.log(len(c)) + 2*res_nll\n",
    "\n",
    "        #store in dataframe\n",
    "        df.at[index, 'alpha_pos'] = param_fits[0]\n",
    "        df.at[index, 'alpha_neg'] = param_fits[1]\n",
    "        df.at[index, 'theta'] = param_fits[2]\n",
    "        df.at[index, 'NLL'] = res_nll\n",
    "\n",
    "        # print/plot Q values for subject   \n",
    "        print(fr'subject {index+1:02}: alpha={param_fits[0]:.2f}, theta={param_fits[1]:.2f}; negLL={res_nll:.2f}; BIC={BIC:.2f}')\n",
    "        nll = model(param_fits, c, o, block, plot=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run\n",
    "# df2 = fitModel2(model=Valence_2a1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-maple",
   "metadata": {},
   "source": [
    "<hr width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model3 (8 out of 30 points)\n",
    "def Social_1a2b1t(params, choices, outcomes, block, plot=False):\n",
    "    # 1 alpha + 1 beta_self + 1 beta_other + 1 theta\n",
    "    \n",
    "    return negLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completed\n",
    "def fitModel3(model=Social_1a2b1t):\n",
    "    \n",
    "    #initialize dataframe to store results\n",
    "    df = pd.DataFrame(index=range(1,12), columns=['alpha','beta_self','beta_other','theta','NLL'])\n",
    "\n",
    "    # initialize list of algorithms \n",
    "    # (use one during class, but try a few others on your own)\n",
    "    algorithms = ['L-BFGS-B'] #['Powell','TNC','SLSQP','trust-constr']\n",
    "\n",
    "    # loop through subjects\n",
    "    for index, behavior in enumerate(subject_data.values()):\n",
    "        c, o, block = behavior.true_accuracy, behavior.outcome, behavior.block\n",
    "        bounds = ((0,1),(0,1),(0,1),(0,10))\n",
    "\n",
    "        # gradient descent to minimize neg LL\n",
    "        res_nll = np.inf # set initial neg LL to be inf\n",
    "\n",
    "        # guess several different starting points for alpha\n",
    "        for alpha_guess in np.linspace(0,1,6):\n",
    "            for beta_self_guess in np.linspace(0,1,6):\n",
    "                for beta_other_guess in np.linspace(0,1,6):\n",
    "                    for theta_guess in np.linspace(1,10,6):\n",
    "\n",
    "                        # guesses for alpha, theta will change on each loop\n",
    "                        init_guess = (alpha_guess, \n",
    "                                      beta_self_guess, \n",
    "                                      beta_other_guess, \n",
    "                                      theta_guess)\n",
    "\n",
    "                        for algorithm in algorithms:\n",
    "\n",
    "                            # minimize neg LL\n",
    "                            result = minimize(model,\n",
    "                                              init_guess,\n",
    "                                              (c, o, block),\n",
    "                                              bounds=bounds,\n",
    "                                              method=algorithm)\n",
    "\n",
    "                            # if current negLL is smaller than the last negLL,\n",
    "                            # then store current data\n",
    "                            if result.fun < res_nll and result.success:\n",
    "                                res_nll = result.fun\n",
    "                                param_fits = result.x\n",
    "\n",
    "        # also, compute BIC\n",
    "        BIC = len(init_guess) * np.log(len(c)) + 2*res_nll\n",
    "\n",
    "        #store in dataframe\n",
    "        df.at[index, 'alpha'] = param_fits[0]\n",
    "        df.at[index, 'beta_self'] = param_fits[1]\n",
    "        df.at[index, 'beta_other'] = param_fits[2]\n",
    "        df.at[index, 'theta'] = param_fits[3]\n",
    "        df.at[index, 'NLL'] = res_nll\n",
    "\n",
    "        # print/plot Q values for subject   \n",
    "        print(fr'subject {index+1:02}: alpha={param_fits[0]:.2f}, theta={param_fits[1]:.2f}; negLL={res_nll:.2f}; BIC={BIC:.2f}')\n",
    "        nll = model(param_fits, c, o, block, plot=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run\n",
    "# df3 = fitModel3(model=Social_1a2b1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-bicycle",
   "metadata": {},
   "source": [
    "<hr width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus (not required for exercise)\n",
    "def SocialValence_4a1t(params, choices, outcomes, block, plot=False):\n",
    "    # 1 alpha_self_pos + 1 alpha_self_neg +\n",
    "    # 1 alpha_other_pos + 1 alpha_other_neg + 1 theta\n",
    "    \n",
    "    return negLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus (not required for exercise)\n",
    "def fitModel4(model=SocialValence_4a1t):\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus (not required for exercise)\n",
    "# Uncomment to run\n",
    "# df4 = fitModel4(model=Social_1a2b1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-assets",
   "metadata": {},
   "source": [
    "<hr width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-venezuela",
   "metadata": {},
   "source": [
    "Now, we should have run 5-6 models that we can compare to assess which model might explain the data best. Remember we store the parameter fits in Pandas DataFrames: `df0a`, `df0b`, `df1`, `df2`, `d3`, and maybe `df4` (if you completed the bonus).\n",
    "\n",
    "Let's use this information to compute Bayesian Information Criterion ($BIC$) across subjects for each model. One way in which we can do so is by summing the negative loglikelihoods across subjects for each model and using that value to compute a $BIC$ score. Then, we can plot each $BIC$ to assess model fits. Please do this below.\n",
    "\n",
    "***Hints***\n",
    "* to compute the sum of the negative likelihoods for the first model, you would use the command: `np.nansum(df0a['NLL'].values)`.\n",
    "* You can refer to [this page](https://shawnrhoads.github.io/gu-psyc-347/module-02-01_Nonlinear-Modeling.html#model-comparison) to review Bayesian Information Criterion ($BIC$), but note that tutorial used $MSE$ to compute the $BIC$. Here, we will use the negative loglikelihood (like [in this tutorial](https://shawnrhoads.github.io/gu-psyc-347/module-03-01_Models-of-Learning.html#gradient-descent)). Remember to factor in the number of free parameters in each of your models!\n",
    "* Remember we will have 5-6 $BIC$ values by the end of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BIC for each Model and Plot (6 out of 30 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-reasoning",
   "metadata": {},
   "source": [
    "Which of these models fit the data best? Think about whether there be others that could do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-slave",
   "metadata": {},
   "source": [
    "<hr width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, please convert this cell to a Markdown cell.\n",
    "\n",
    "# Create a Heading named \"Notebook Feedback,\" then delete this text \n",
    "# and provide 1-2 sentences about your experience with this \n",
    "# Jupyter Notebook (e.g., Did you enjoy the exercises? Were \n",
    "# they too easy/difficult? Would you have like to see \n",
    "# anything different? Were you able to apply some skills we learned during\n",
    "# class? Anything still confusing?). Finally, please rate your experience\n",
    "# from (0) \"did not enjoy at all\" to (10) \"enjoyed a great deal.\" Only your\n",
    "# instructor will see these responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-jersey",
   "metadata": {},
   "source": [
    "Please save this file as an .html file by clicking on **'File > Download As > HTML (.html)'**\n",
    "\n",
    "Then, save the file as **\"Lastname_Exercise04.html\"** and submit on Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

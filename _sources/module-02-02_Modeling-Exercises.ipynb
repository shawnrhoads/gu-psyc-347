{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Anconda, please re-download this file to current directory:\n",
    "# https://raw.githubusercontent.com/shawnrhoads/gu-psyc-347/master/course-env.yml\n",
    "# Then, run this cell\n",
    "# !conda env update --file course-env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-hearing",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shawnrhoads/gu-psyc-347/blob/master/docs/module-02-02_Modeling-Exercises.ipynb)\n",
    "\n",
    "# Modeling Exercises\n",
    "\n",
    "These exercises were inspired by and adapted from [Neuromatch Academy](https://github.com/NeuromatchAcademy/course-content) ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)) and [DartBrains](https://dartbrains.org) ([CC BY-SA 4.0 License](https://creativecommons.org/licenses/by-sa/4.0/)).\n",
    "\n",
    "\n",
    "We will continue to use data from [O'Connell, K., Berluti, K., Rhoads, S. A., & Marsh, A. A. (2021). Reduced social distancing during the COVID-19 pandemic is associated with antisocial behaviors in an online United States sample. PLoS ONE.](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0244974)\n",
    "\n",
    "This study <u>used a linear model</u> to assess whether social distancing behaviors (early in the COVID-19 pandemic) was associated with self-reported antisocial behavior. To measure one index of social distancing behavior, participants were presented with an image of an adult silhouette surrounded by a rectangular border. They were asked to click a point in the image that represents how far away they typically stood from other individuals. \n",
    "\n",
    "Here is a heatmap showing how far participants reported standing from other individuals in the past week, with dark maroon indicating a higher density of responses obtained from a kernel density estimation. The mean response coordinate, +, represents a distance of approximately 98 inches (8.2 feet; 2.5 m).\n",
    "![Figure 1](https://journals.plos.org/plosone/article/figure/image?download&size=medium&id=info:doi/10.1371/journal.pone.0244974.g001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-detroit",
   "metadata": {},
   "source": [
    "For this exercise, we will reproduce the results from O'Connell et al. (2021) that distance kept from others in the past week can be expressed as a function of antisocial behavior, controlling for age, sex, education, household income, high-risk status, and PPE use frequency. \n",
    "\n",
    "We will reproduce the beta coefficients from **Table 3** in the paper.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Exploring the Data](#Exploring-the-Data)\n",
    "2. [Ordinary Least Squares](#Ordinary-Least-Squares)\n",
    "3. [Maximum Likelihood Estimation](#Maximum-Likelihood-Estimation)\n",
    "\n",
    "#### Key\n",
    "- `# [INSERT CODE BELOW]` : indicates where you should insert your own code, feel free to replace with a comment of your own\n",
    "- `...`: indicates a location where you should insert your own code\n",
    "- `raise NotImplementedError(\"Student exercise: *\")` : delete this line once you have added your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-reynolds",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will import all of our packages for the exercise\n",
    "\n",
    "from scipy.optimize import minimize # finding optimal params in models\n",
    "from scipy import stats             # statistical tools\n",
    "import numpy as np                  # matrix/array functions\n",
    "import pandas as pd                 # loading and manipulating data\n",
    "import requests                     # downloading data from the internet\n",
    "import ipywidgets as widgets        # interactive display\n",
    "import matplotlib.pyplot as plt     # plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please load in the data into a pandas DataFrame, \n",
    "# then print the first five rows to check that everything loaded correctly\n",
    "# hint: we've done this in previous exercises\n",
    "\n",
    "# [INSERT CODE BELOW]\n",
    "raise NotImplementedError(\"Student exercise: load OConnell_COVID_MTurk_noPII_post_peerreview.csv file as pandas dataframe, then delete this line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-necklace",
   "metadata": {},
   "source": [
    "Oh no! Our data contain NaNs (participants either did not answer the question, or did not meet inclusion criteria for certain variables). Let's get rid of them using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['age_centered',\n",
    "             'sex_1f',\n",
    "             'education_4yr',\n",
    "             'household_income_coded_centered',\n",
    "             'highrisk_self_or_livewith',\n",
    "             'ppe_freq_coded_2',\n",
    "             'STAB_total_centered',\n",
    "             'silhouette_dist_X_min81']\n",
    "\n",
    "df.dropna(subset=var_names,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-senate",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "Now that we've loaded in our data, let's explore their underlying distributions and then fit these data to a linear model by minimizing the mean squared error (MSE)--this is also known as ordinary least squares.\n",
    "\n",
    "This model can be expressed mathematically as:\n",
    "\n",
    "$\n",
    "Distancing = \\beta_0 + \\beta_1 \\cdot age + \\beta_2 \\cdot sex + \\beta_3 \\cdot edu + \\beta_4 \\cdot income + \\beta_5 \\cdot risk + \\beta_6 \\cdot PPE + \\beta_7 \\cdot antisociality\n",
    "$\n",
    "\n",
    "Our goal is to estimate the $\\beta$ parameters that explain the relationships in our model.\n",
    "\n",
    "To accomplish this, we need to do the following:\n",
    "1. Convert the dependent variable column (1) in the pandas DataFrameto a numpy array `y` with size `(n_subjects, 1)`.\n",
    "2. Convert the independent variable columns (7) in the pandas DataFrame to a numpy array `X` with size `(n_subjects, 8)`, the first column should represent an intercept and contain ones.\n",
    "3. Define a function `ols()` that takes inputs `X` (numpy array) and `y` (numpy array), and outputs `params` (list or numpy array).\n",
    "4. Run the function and find the optimal parameter estimates that fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of subjects and create X and y\n",
    "\n",
    "# [INSERT CODE BELOW]\n",
    "raise NotImplementedError(\"Student exercise: initialize n_subjects, X and y, then delete this line\")\n",
    "\n",
    "n_subjects = len(...)\n",
    "\n",
    "# store columns of interest in a single numpy array \n",
    "    # age_centered - age (continuous; 18-65)\n",
    "    # sex_1f - sex (0: male; 1: female)\n",
    "    # education_4yr - education (0: < 4-year degree; 1: >= 4-year degree)\n",
    "    # household_income_coded_centered - household income (continuous)\n",
    "    # highrisk_self_or_livewith - high-risk for serious illness for self or someone the participant lives with (0: no; 1: yes)\n",
    "    # ppe_freq_coded_2 - PPE use frequency (continuous; 1: Never, 5: Always)\n",
    "    # stab_total_centered - antisocial behavior (continuous)\n",
    "    # silhouette_dist_x_min81 - distance kept from others in past week (continuous)\n",
    "\n",
    "x0 = np.ones((n_subjects,))\n",
    "x1 = ... # age\n",
    "x2 = ... # sex\n",
    "x3 = ... # edu\n",
    "x4 = ... # income\n",
    "x5 = ... # high-risk\n",
    "x6 = ... # ppe\n",
    "x7 = ... # antisociality (stab)\n",
    "\n",
    "X = np.stack((...), axis=1)\n",
    "y = ... # distance from others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a for loop, please print the means and standard deviations of these variables\n",
    "\n",
    "# [INSERT CODE BELOW]\n",
    "raise NotImplementedError(\"Student exercise: plot histograms of the variables, then delete this line\")\n",
    "\n",
    "for name, data in zip(var_names, ...):\n",
    "    print(f'{name}: mean={np.mean(data):.2f}, sd={np.std(data):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function called ols() that computes betas for the model\n",
    "# no need to edit anything here\n",
    "# see module-02-00_Linear-Modeling for derivation\n",
    "\n",
    "def ols(X, y):\n",
    "    beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the betas\n",
    "# [INSERT CODE BELOW]\n",
    "raise NotImplementedError(\"Student exercise: compute beta_hats using the function we just defined, then delete this line\")\n",
    "    \n",
    "beta_hats = ols(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-cotton",
   "metadata": {},
   "source": [
    "Now, that we have our fitted parameters, please print our 8 coefficients corresponding to our variables in a for loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the names of each variable\n",
    "x_vars = ['intercept', 'age','sex','edu','income','high-risk','ppe','antisociality']\n",
    "\n",
    "for variable, beta_value in zip(x_vars, ...):\n",
    "    # [INSERT CODE BELOW]\n",
    "    raise NotImplementedError(\"Student exercise: complete the string below, then delete this line\")\n",
    "    \n",
    "    print(fr'coefficient for {...} = {...:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-console",
   "metadata": {},
   "source": [
    "Awesome! We were able to recover the beta coefficients from the model in O'Connell et al. (2021) using least-squares minimization (which has an analytic solution). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-scottish",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-south",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "Above, we made the assumption that the data were drawn from a linear relationship with noise added, and found an effective approach for estimating model parameters based on minimizing the mean squared error.\n",
    "\n",
    "We treated the noise as simply a nuisance, but what if we factored it directly into our model?\n",
    "\n",
    "Recall our linear model:\n",
    "\n",
    "$$\n",
    "y = \\beta \\cdot x + \\epsilon.\n",
    "$$\n",
    "\n",
    "The noise component $\\epsilon$ is often modeled as a random variable drawn from a Gaussian distribution (also called the normal distribution).\n",
    "\n",
    "The Gaussian distribution is described by its [probability density function](https://en.wikipedia.org/wiki/Probability_density_function) (pdf): \n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\n",
    "$$\n",
    "\n",
    "and is dependent on two parameters: the mean $\\mu$ and the variance $\\sigma^2$. We often consider the noise signal to be Gaussian \"white noise\", with zero mean and unit variance:\n",
    "\n",
    "$$\n",
    "\\epsilon \\sim \\mathcal{N}(0, 1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-lobby",
   "metadata": {},
   "source": [
    "Before we begin, use the widget below to see how varying the $\\mu$ and $\\sigma$ parameters change the location and shape of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "@widgets.interact(mu=widgets.FloatSlider(0.0, min=-2.0, max=2.0),\n",
    "                  sigma=widgets.FloatSlider(1.0, min=0.5, max=2.0))\n",
    "\n",
    "def plot_normal_dist(mu=0, sigma=1):\n",
    "\n",
    "  # Generate pdf & samples from normal distribution with mu/sigma\n",
    "  rv = stats.norm(mu, sigma)\n",
    "  x = np.linspace(-5, 5, 100)\n",
    "  y = rv.pdf(x)\n",
    "  samples = rv.rvs(1000)\n",
    "\n",
    "  # Plot\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.hist(samples, 20, density=True, color='g', histtype='stepfilled', alpha=0.8,\n",
    "          label='histogram')\n",
    "  ax.plot(x, y, color='orange', linewidth=3, label='pdf')\n",
    "  ax.vlines(mu, 0, rv.pdf(mu), color='y', linewidth=3, label='$\\mu$')\n",
    "  ax.vlines([mu-sigma, mu+sigma], 0, rv.pdf([mu-sigma, mu+sigma]), colors='red',\n",
    "            color='b', linewidth=3, label='$\\sigma$')\n",
    "  ax.set(xlabel='x', ylabel='probability density', xlim=[-5, 5], ylim=[0, 1.0])\n",
    "  ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-talent",
   "metadata": {},
   "source": [
    "Now that we have a model of our noise component $\\epsilon$ as random variable, how do we incorporate this back into our original linear model from before? Consider again our simplified model $y = \\beta \\cdot x + \\epsilon$ where the noise has zero mean and unit variance $\\epsilon \\sim \\mathcal{N}(0, 1)$. We can now also treat $y$ as a random variable drawn from a Gaussian distribution where $\\mu = \\beta \\cdot x$ and $\\sigma^2 = 1$:\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}(\\beta \\cdot x, 1)\n",
    "$$\n",
    "\n",
    "which is to say that the probability of observing $y$ given $x$ and parameter $\\beta$ is\n",
    "\n",
    "$$\n",
    "p(y|x,\\beta) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y-\\beta \\cdot x)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-paraguay",
   "metadata": {},
   "source": [
    "Now that we have our probabilistic model, we turn back to our original challenge of finding a good estimate for $\\beta$ that fits our data. Given the inherent uncertainty when dealing in probabilities, we talk about the [likelihood](https://en.wikipedia.org/wiki/Likelihood_function) that some estimate $\\hat \\beta$ fits our data. The likelihood function $\\mathcal{L(\\beta)}$ is equal to the probability density function parameterized by that $\\theta$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta|x,y) = p(y|x,\\beta) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y-\\beta \\cdot x)^2}\n",
    "$$\n",
    "\n",
    "Let's implement the likelihood function $\\mathcal{L}(\\beta|x,y)$ for our linear model where $\\sigma = 1$.\n",
    "\n",
    "After implementing this function, we can produce probabilities that our estimate $\\hat{\\beta}$ generated the provided observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(beta_hat, X, y):\n",
    "    \"\"\"The likelihood function for a linear model with noise sampled from a Gaussian distribution with zero mean and unit variance.\n",
    "\n",
    "    Args:\n",
    "    beta_hat (float): Estimates of the slope parameters.\n",
    "    x (ndarray): An array of shape (samples,) that contains the input values.\n",
    "    y (ndarray): An array of shape (samples,) that contains the corresponding measurement values to the inputs.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: the likelihood values for the beta_hat estimates\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute the predicted y based on inputted beta_hats\n",
    "    # here, dot product is equivalent to the weighted sum of values \n",
    "    # (i.e., b0 + b1*x1 + b2*x2 + .. bd*xd)\n",
    "    predicted_y = np.dot(X, beta_hat) \n",
    "\n",
    "    # Compute Gaussian likelihood\n",
    "    pdf = stats.norm.logpdf(y, loc=predicted_y)\n",
    "    \n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-belle",
   "metadata": {},
   "source": [
    "### Finding the Maximum Likelihood Estimator\n",
    "\n",
    "We want to find the parameter value $\\hat\\beta$ that makes our data set most likely:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\textrm{MLE}} = \\underset{\\beta}{\\operatorname{argmax}} \\mathcal{L}(\\beta|X,Y) \n",
    "$$\n",
    "\n",
    "We discussed how taking the logarithm of the likelihood helps with numerical stability, the good thing is that it does so without changing the parameter value that maximizes the likelihood. Indeed, the $\\textrm{log}()$ function is *monotonically increasing*, which means that it preserves the order of its inputs. So we have:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\textrm{MLE}} = \\underset{\\beta}{\\operatorname{argmax}} \\sum_{i=1}^m \\textrm{log} \\mathcal{L}(\\beta|x_i,y_i) \n",
    "$$\n",
    "\n",
    "Now substituting our specific likelihood function and taking its logarithm, we get:\n",
    "$$\n",
    "\\hat{\\beta}_{\\textrm{MLE}} = \\underset{\\beta}{\\operatorname{argmax}} [-\\frac{N}{2} \\operatorname{log} 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i-\\beta \\cdot x_i)^2].\n",
    "$$\n",
    "\n",
    "Note that maximizing the log likelihood is the same as **minimizing the negative log likelihood** (in practice optimization routines are developed to solve minimization not maximization problems). Because of the convexity of this objective function, we can take the derivative of our negative log likelihhood, set it to 0, and solve - just like our solution to minimizing MSE.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\operatorname{log}\\mathcal{L}(\\beta|x,y)}{\\partial\\beta}=\\frac{1}{\\sigma^2}\\sum_{i=1}^N(y_i-\\beta \\cdot x_i)x_i = 0\n",
    "$$\n",
    "\n",
    "Note that this looks remarkably like the equation we had to solve for the optimal MSE estimator, and, in fact, we arrive to the exact same solution!\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\textrm{MLE}} = \\hat{\\beta}_{\\textrm{MSE}} = \\frac{\\sum_{i=1}^N x_i y_i}{\\sum_{i=1}^N x_i^2}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\hat\\beta = \\frac{\\vec{x}^\\top \\vec{y}}{\\vec{x}^\\top \\vec{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-emerald",
   "metadata": {},
   "source": [
    "Even though we already know the analytic solution to solve for our $\\hat{\\beta}$ coefficients, let's continue on to solve for them bu minimizing the negative loglikelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please compute the negative loglikelihood in this new function\n",
    "# note: this function calls the likelihood() function we defined earlier\n",
    "# hint: use the function: np.sum()\n",
    "def minimize_negll(params, X, y):\n",
    "    \"\"\"The likelihood function for a model with noise sampled from a\n",
    "    Gaussian distribution with zero mean and unit variance.\n",
    "\n",
    "    Args:\n",
    "    params (list)\n",
    "    X (ndarray): An array of shape (samples,) that contains the input values.\n",
    "    y (ndarray): An array of shape (samples,) that contains the corresponding\n",
    "      measurement values to the inputs.\n",
    "\n",
    "    Returns:\n",
    "    scalar: the neg sum of the loglikelihoods for the beta_hat estimate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute Gaussian loglikelihood\n",
    "    loglik = loglikelihood(params, X, y)\n",
    "    \n",
    "    # [INSERT CODE BELOW]\n",
    "    raise NotImplementedError(\"Student exercise: compute the negative sum of the loglikelihoods computed above, then delete this line\")\n",
    "    \n",
    "    negLL = ... \n",
    "    \n",
    "    return negLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify starting points\n",
    "init_x = (1,1,1,1,1,1,1,1) # can be anything\n",
    "\n",
    "# [INSERT CODE BELOW]\n",
    "raise NotImplementedError(\"Student exercise: complete the code below to find the optimal params, then delete this line\")\n",
    "\n",
    "# minimize MSE for linear function using scipy.optimize.minimize\n",
    "mle_results = minimize(..., # objective function\n",
    "                       init_x, # estimated starting points\n",
    "                       args=(..., ...)) # arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-sheffield",
   "metadata": {},
   "source": [
    "Now, that we have our fitted parameters (now using MLE), please print our 8 coefficients corresponding to our variables in a for loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the names of each variable\n",
    "x_vars = ['intercept', 'age', 'sex', 'edu', 'income', 'high-risk', 'ppe', 'antisociality']\n",
    "\n",
    "# [INSERT CODE BELOW]\n",
    "raise NotImplementedError(\"Student exercise: complete the code below, then delete this line\")\n",
    "\n",
    "# think about where you stored the results of the minimization\n",
    "mle_betas = ...\n",
    "\n",
    "for variable, beta_value in zip(x_vars, mle_betas):\n",
    "    \n",
    "    print(fr'coefficient for {...} = {...:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-glenn",
   "metadata": {},
   "source": [
    "Wow, how cool is that?! We can recover the same parameters by minimizing the loglikelihood (as we did earlier using least squares minimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-shame",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-patent",
   "metadata": {},
   "source": [
    "## Bonus: Explained Variance\n",
    "\n",
    "Sometimes we want a single metric to quantify overall how well our model was able to explain variance in the data. There are many metrics that can provide a quantitative measure of *goodness of fit*.\n",
    "\n",
    "Here we will calculate $R^2$ using the following formula:\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\displaystyle \\sum_i^n(\\hat y_i - y_i)^2}{\\displaystyle \\sum_i^n(y_i - \\bar y)^2}$$\n",
    "\n",
    "where $y_i$ is the measured value of the voxel at timepoint $i$, $\\hat y_i$ is the predicted value for time point $i$, and $\\bar y$ is the mean of the measured voxel timeseries.\n",
    "\n",
    "$R^2$ will lie on the interval between $[0,1]$ and can be interpreted as percentage of the total variance in $Y$ explained by the model, $X$, where 1 is 100% and 0 is none.\n",
    "\n",
    "$R^2$ is also used to compare model fits, similar to how we $BIC$ used in the nonlinear modeling tutorial (see Table 3 in paper, which assessed the difference in $R^2$ across models to select the best-fitting model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y, predicted_y):\n",
    "    SS_residual = np.sum((y - predicted_y)**2)\n",
    "    SS_total = np.sum((y - np.mean(y))**2)\n",
    "    return 1-(SS_residual/SS_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = np.dot(X, mle_results.x)\n",
    "print(f\"R^2: {r_squared(y, predicted_y):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-brief",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, please convert this cell to a Markdown cell.\n",
    "\n",
    "# Create a Heading named \"Notebook Feedback,\" then delete this text \n",
    "# and provide 1-2 sentences about your experience with this \n",
    "# Jupyter Notebook (e.g., Did you enjoy the exercises? Were \n",
    "# they too easy/difficult? Would you have like to see \n",
    "# anything different? Were you able to apply some skills we learned during\n",
    "# class? Anything still confusing?). Finally, please rate your experience\n",
    "# from (0) \"did not enjoy at all\" to (10) \"enjoyed a great deal.\" Only your\n",
    "# instructor will see these responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-advice",
   "metadata": {},
   "source": [
    "Please save this file as an .html file by clicking on **'File > Download As > HTML (.html)'**\n",
    "\n",
    "Then, save the file as **\"Lastname_Exercise01.html\"** and submit on Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
